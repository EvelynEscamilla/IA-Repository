{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping Google parte 2\n",
    "\n",
    "Este código utiliza selenium y BeautifulSoup para automatizar la extracción de información de páginas web, convirtiéndola en texto sin etiquetas HTML, y luego guarda los resultados en archivos de texto individuales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selenium: Automatiza el navegador para interactuar con páginas web.\n",
    "\n",
    "webdriver_manager: Administra automáticamente el controlador del navegador Chrome.\n",
    "\n",
    "BeautifulSoup: Extrae y procesa el contenido HTML de las páginas.\n",
    "\n",
    "sleep: Pausa la ejecución para permitir la carga completa de la página.\n",
    "\n",
    "json: Permite leer el archivo enlaces_reforma_judicial.json y guardar datos en un formato estructurado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurar Selenium:\n",
    "Utiliza un navegador Chrome en modo sin interfaz gráfica (--headless) para mayor eficiencia.\n",
    "\n",
    "\n",
    "Acceder a la URL:\n",
    "Abre la página web con driver.get(url).\n",
    "\n",
    "\n",
    "Esperar carga de contenido:\n",
    "Usa sleep(3) para dar tiempo a que se cargue la página.\n",
    "\n",
    "\n",
    "Procesar HTML con BeautifulSoup:\n",
    "Extrae el contenido de la página con get_text, eliminando etiquetas HTML.\n",
    "\n",
    "\n",
    "Cerrar navegador:\n",
    "Garantiza que el navegador se cierre con driver.quit().\n",
    "\n",
    "\n",
    "Retornar resultados:\n",
    "Devuelve un diccionario con la URL y el texto extraído, o un error si ocurre un problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_info_pagina(url):\n",
    "    try:\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")  \n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        \n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "        \n",
    "        driver.get(url)\n",
    "        \n",
    "        sleep(3)\n",
    "        \n",
    "        html = driver.page_source\n",
    "        \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        texto_sin_etiquetas = soup.get_text(separator=\"\\n\", strip=True)\n",
    "        \n",
    "        driver.quit()\n",
    "        \n",
    "        return {'url': url, 'texto_sin_etiquetas': texto_sin_etiquetas}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {'url': url, 'error': str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abrir archivo de texto:\n",
    "Usa open en modo escritura ('w') y codificación UTF-8 para soportar caracteres especiales.\n",
    "\n",
    "\n",
    "Escribir información:\n",
    "Incluye la URL y el texto extraído sin etiquetas HTML.\n",
    "\n",
    "\n",
    "Guardar y notificar:\n",
    "Muestra un mensaje indicando el nombre del archivo guardado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_en_txt(informacion, nombre_archivo):\n",
    "    with open(nombre_archivo, 'w', encoding='utf-8') as archivo:\n",
    "        archivo.write(f\"Información extraída de la URL: {informacion['url']}\\n\\n\")\n",
    "        archivo.write(f\"Texto extraído sin etiquetas HTML:\\n\")\n",
    "        archivo.write(f\"{informacion['texto_sin_etiquetas']}\\n\\n\")\n",
    "        print(f\"Información guardada en {nombre_archivo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leer enlaces desde JSON:\n",
    "Abre y carga el archivo enlaces_reforma_judicial.json. Si no existe, muestra un mensaje de error.\n",
    "\n",
    "Validar enlaces:\n",
    "Si no hay enlaces, finaliza la ejecución.\n",
    "\n",
    "Procesar cada enlace:\n",
    "Llama a extraer_info_pagina para cada enlace y guarda el resultado en un archivo de texto único.\n",
    "\n",
    "Manejar errores:\n",
    "Si ocurre un error al extraer información de una página, lo notifica en la consola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        with open('enlaces_reforma_judicial.json', 'r', encoding='utf-8') as f:\n",
    "            links = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"No se encontraron enlaces guardados.\")\n",
    "        return\n",
    "\n",
    "    if not links:\n",
    "        print(\"No se encontraron enlaces.\")\n",
    "        return\n",
    "    \n",
    "    for idx, link in enumerate(links, 1):\n",
    "        info_pagina = extraer_info_pagina(link)\n",
    "        if 'error' in info_pagina:\n",
    "            print(f\"Error al extraer la información de {link}: {info_pagina['error']}\")\n",
    "        else:\n",
    "            archivo_txt = f'informacion_extraida_{idx}.txt' \n",
    "            guardar_en_txt(info_pagina, archivo_txt)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
